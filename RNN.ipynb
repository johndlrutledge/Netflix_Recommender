{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"i9H2df0Sv5X-"},"outputs":[],"source":["# uncomment to install requirements\n","#!pip install tqdm pandas keras-tuner scikit-learn tensorflow matplotlib seaborn numpy ipykernel jupyter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEIm3yIoC3Dw"},"outputs":[],"source":["# Work in progress place to put the main code\n","# TODO next: reorganize into time windows by user and ordered by data ()\n","import tqdm\n","import pandas as pd\n","import numpy as np\n","import keras_tuner as kt\n","from tensorflow.keras.layers import Embedding, Dense, LSTM, GRU\n","from tensorflow.keras import Input, Model\n","from sklearn.model_selection import train_test_split\n","from random import choice, sample\n","import tensorflow as tf\n","from gc import collect\n","#import multiprocessing\n","from pickle import dump, load\n","#pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n","#pool = multiprocessing.Pool(processes=6)\n","\n","# try:\n","#   tf.keras.mixed_precision.set_global_policy('mixed_float16')\n","# except:\n","#   pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOXm4qcGC3Dy","outputId":"6eba7686-1f18-4e00-a1c0-6ea3f7950965"},"outputs":[{"data":{"text/plain":["TensorShape([9000, 181, 1])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["DATA_FILE_NAME = 'top_data_dimreduce_20.csv'\n","def load_data():\n","    \"\"\"Takes one hour to run (single thread), so it is probably best to downlaod user.pkl\n","\n","       File format is for local machine. Alter if running on colab\n","    \"\"\"\n","    try:\n","        with open(r\"users.pkl\", \"rb\") as f:\n","            users = load(f)\n","    except:\n","        data = pd.read_csv(DATA_FILE_NAME, sep=',', header=0, names=['Movie','User','Rating','Date','Rating_uco','Rating_mco'])\n","        data['Date'] = pd.to_datetime(data['Date'])\n","        data['Date'] = data['Date'].astype('int64')/max(data['Date'].astype('int64'))\n","        data = data.dropna(axis=0)\n","        total_ratings = data.shape[0]\n","        print(f\"Total number of ratings: {total_ratings}\")\n","        data.sort_values(['User','Date'],inplace=True)\n","        data.head()\n","        users = {i:[] for i in set(data['User'])}\n","        sett = [[data.iloc[rating].User,[data.iloc[rating].Date, data.iloc[rating].Movie, data.iloc[rating].Rating_uco, data.iloc[rating].Rating_mco, data.iloc[rating].Rating]] for rating in range(total_ratings)]\n","        for [user, settt] in sett: users[user].append(settt)\n","        with open(r\"users.pkl\", \"wb\") as f:\n","            dump(users, f)\n","        del data\n","        del settt,sett\n","    return users\n","\n","class rate_movies:\n","    def __init__(self):\n","        self.bs = 100\n","        self.lr = 0.01\n","        self.model = None\n","        self.data = None\n","        self.train_data_x = None\n","        #self.val_data_x = None\n","        self.test_data_x = None\n","        self.train_data_y = None\n","        #self.val_data_y = None\n","        self.test_data_y = None\n","        self.seq_length = 10\n","        self.buffer_size = 10000\n","        self.seed = 897\n","        self.model = None\n","        self.tuned_model = None\n","        self.tuner = None\n","        self.split_ratios = {'train':9/10,'test':1} # the iterative percentage of data to remove in sequence. Result: 70-20-10 split *Edit: val dataset is now split during fit\n","    \n","    def split_load_data(self):\n","        train_needed = False\n","        try: # try to load the data. Not stylistic method.\n","            with open(r\"trainx.pkl\", \"rb\") as f:\n","                self.train_data_x = load(f)\n","            #with open(r\"valx.pkl\", \"rb\") as f:\n","                #self.val_data_x = load(f)\n","            with open(r\"testx.pkl\", \"rb\") as f:\n","                self.test_data_x = load(f)\n","            with open(r\"trainy.pkl\", \"rb\") as f:\n","                self.train_data_y = load(f)\n","            #with open(r\"valy.pkl\", \"rb\") as f:\n","                #self.val_data_y = load(f)\n","            with open(r\"testy.pkl\", \"rb\") as f:\n","                self.test_data_y = load(f)\n","\n","            #self.train_data_x, self.train_data_y, self.val_data_x, self.val_data_y, self.test_data_x, self.test_data_y  = tf.ragged.constant(self.train_data_x), tf.ragged.constant(self.train_data_y), tf.ragged.constant(self.val_data_x), tf.ragged.constant(self.val_data_y), tf.ragged.constant(self.test_data_x), tf.ragged.constant(self.test_data_y)\n","\n","        except: # process the data and then create dump files if we cannot load\n","            train_needed = True\n","\n","        if train_needed:\n","            self.data = load_data()\n","            selection = []\n","            i = 0 #keeps track of what split ratio to apply\n","            for ratio in tqdm.tqdm(self.split_ratios):\n","                selection.append(sample(sorted(self.data), k=int(len(self.data)*self.split_ratios[ratio]))) #random sample of the users (selection means all user data goes to that dataset)\n","                keys = selection[i]  # copy user numbers bc we are about to replace it with their data\n","                selection[i] = [self.data[x] for x in selection[i]] \n","                i+=1  # next dataset\n","                for key in keys: # for every username in the sample, remove it from the selection pool\n","                    if key in self.data: del self.data[key]\n","\n","            train_data, test_data = selection  # split list set into the datasets\n","            #del selection, self.data, i, keys # saves memory... remove if needed.\n","\n","            def labels(input):  # probably working now\n","                # input = [[(input[:-1],[input[-1]]) for input in input]  for input in input]  # Needed: create window... last element of window must become label... convert to tenseors... batch\n","                i = 0\n","                c = len(input)\n","                h = 0\n","                x,y = [],[]\n","\n","                for [date, movie, r, rr, rating] in input:\n","                    h+=1\n","                    if h == c:\n","                        x[-1]=x[-1]+[date, movie, r, rr]\n","                        y.append([rating])\n","                        #x[-1],y[-1] = tf.ragged.constant(x[-1]), tf.ragged.constant(y[-1])\n","                        i = 0\n","                    elif i == 0:\n","                        x.append([date, movie, r, rr, rating])\n","                        i+=1\n","                    elif i < self.seq_length:\n","                        x[-1]=x[-1]+[date, movie, r, rr, rating]\n","                        i += 1\n","                    else:\n","                        x[-1]=x[-1]+[date, movie, r, rr]\n","                        y.append([rating])\n","                        #x[-1],y[-1] = tf.ragged.constant(x[-1]), tf.ragged.constant(y[-1])\n","                        i = 0\n","                #dataset = zip(tf.data.Dataset.from_tensor_slices(x).batch(self.bs, drop_remainder=True), tf.data.Dataset.from_tensor_slices(y).batch(self.bs, drop_remainder=True))\n","                #del x,y,i,h,c\n","                return (x,y)\n","\n","            def iterate(input):\n","                x = []\n","                y=[]\n","                for i in tqdm.tqdm(input):\n","                    ii,iii = labels(i)\n","                    x.append(ii)\n","                    y.append(iii)\n","                return x,y\n","\n","            self.train_data_x, self.train_data_y = iterate(train_data)\n","            #self.val_data_x, self.val_data_y   = iterate(val_data)\n","            self.test_data_x, self.test_data_y  = iterate(test_data)\n","            self.train_data_x, self.train_data_y, self.test_data_x, self.test_data_y  = tf.ragged.constant(self.train_data_x).to_tensor(), tf.ragged.constant(self.train_data_y).to_tensor(), tf.ragged.constant(self.test_data_x).to_tensor(), tf.ragged.constant(self.test_data_y).to_tensor()\n","\n","            with open(r\"trainx.pkl\", \"wb\") as f:\n","                dump(self.train_data_x, f)\n","            # with open(r\"valx.pkl\", \"wb\") as f:\n","            #     dump(self.val_data_x, f)\n","            with open(r\"testx.pkl\", \"wb\") as f:\n","                dump(self.test_data_x, f)\n","            with open(r\"trainy.pkl\", \"wb\") as f:\n","                dump(self.train_data_y, f)\n","            # with open(r\"valy.pkl\", \"wb\") as f:\n","            #     dump(self.val_data_y, f)\n","            with open(r\"testy.pkl\", \"wb\") as f:\n","                dump(self.test_data_y, f)\n","\n","\n","    def recc_model(self):\n","        def model_arch(layers=2, units=228, dropout_rate=0.2):\n","            inputs = tf.keras.Input(batch_input_shape=[self.bs, None, 58])\n","            x = tf.keras.layers.Normalization()(inputs)\n","            for _ in range(layers): \n","                x = LSTM(units, return_sequences=True, stateful=True)(x)\n","            if dropout_rate > 0.0: x = tf.keras.layers.Dropout(dropout_rate)(x)\n","            x = Dense(1, activation='swish') (x)\n","            model = tf.keras.Model(inputs,x)\n","            loss= tf.keras.optimizers.Adam()\n","            model.compile(optimizer=loss, loss='mse',metrics=tf.keras.metrics.RootMeanSquaredError(name='root_mean_squared_error'))\n","            return model\n","        self.model = model_arch()\n","        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)]\n","        self.model.fit(self.train_data_x,self.train_data_y, validation_split=0.3,epochs = 500, callbacks=callbacks,batch_size=self.bs)\n","\n","    def keras_tuner(self,search): #In progress.. too many things rn to test so will eliminate some choices\n","        if search:\n","            def model_arch(hp):\n","                inputs = tf.keras.Input(batch_input_shape=[self.bs, None, 58])\n","                x = tf.keras.layers.Normalization()(inputs)\n","                for _ in range(hp.Int('layers',0,4)): \n","                    x = LSTM(hp.Int('units',1,512), return_sequences=True, stateful=True)(x)\n","                if hp.Float('dropout_rate',0.0,.5) > 0.0: x = tf.keras.layers.Dropout(hp.Float('dropout_rate',0.0,.5))(x)\n","                x = Dense(1, activation=hp.Choice('final_activation',['swish','relu','selu','elu','gelu'])) (x)\n","                model = tf.keras.Model(inputs,x)\n","                loss= tf.keras.optimizers.Adam(learning_rate=hp.Float('lr',0,0.05))\n","                model.compile(optimizer=loss, loss='mse')#,metrics=tf.keras.metrics.RootMeanSquaredError(name='root_mean_squared_error')\n","                return model\n","        else: # return the best model (currently some settings are hard-coded to predicted best values because the search ran out of time)\n","            def model_arch():\n","                inputs = tf.keras.Input(batch_input_shape=[self.bs, None, 58])\n","                x = tf.keras.layers.Normalization()(inputs)\n","                for _ in range(4): \n","                    x = LSTM(319, return_sequences=True, stateful=True)(x)\n","                x = tf.keras.layers.Dropout(0.39063)(x)\n","                x = Dense(1, activation='gelu') (x)\n","                model = tf.keras.Model(inputs,x)\n","                loss= tf.keras.optimizers.Adam(learning_rate=0.00097747)\n","                model.compile(optimizer=loss, loss='mse',metrics=tf.keras.metrics.RootMeanSquaredError(name='root_mean_squared_error'))#,metrics=tf.keras.metrics.RootMeanSquaredError(name='root_mean_squared_error')\n","                return model\n","        if search:\n","            self.tuner = kt.Hyperband(model_arch,objective='val_loss', max_epochs=500, hyperband_iterations=5)\n","            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=True)]\n","            self.tuner.search(self.train_data_x,self.train_data_y, validation_split=0.3,epochs = 50, callbacks=callbacks,batch_size=self.bs)\n","            *best_parameters, = self.tuner.get_best_hyperparameters()[0]\n","            dump(best_parameters,open( \"best_parameters.pkl\", \"wb\" ))\n","        \n","        self.tuned_model = model_arch()\n","        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)]\n","        #self.tuned_model.summary()\n","        self.tuned_model.fit(self.train_data_x,self.train_data_y, validation_split=0.3,epochs = 500, callbacks=callbacks,batch_size=self.bs)\n","\n","        \n","\n","    \n","    def eval(self,model):\n","        model.evaluate(self.test_data_x,self.test_data_y,batch_size=self.bs)\n","        pass\n","\n","\n","rec_system = rate_movies()\n","\n","rec_system.split_load_data()\n","rec_system.train_data_y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PijdU5mC3D9","outputId":"9dcd29c9-9270-4eef-aeb3-469b84637eb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/500\n","63/63 [==============================] - 4s 47ms/step - loss: 0.8338 - root_mean_squared_error: 0.9131 - val_loss: 0.5782 - val_root_mean_squared_error: 0.7604\n","Epoch 2/500\n","63/63 [==============================] - 3s 41ms/step - loss: 0.5814 - root_mean_squared_error: 0.7625 - val_loss: 0.5645 - val_root_mean_squared_error: 0.7513\n","Epoch 3/500\n","63/63 [==============================] - 3s 40ms/step - loss: 0.5657 - root_mean_squared_error: 0.7521 - val_loss: 0.5459 - val_root_mean_squared_error: 0.7389\n","Epoch 4/500\n","63/63 [==============================] - 2s 40ms/step - loss: 0.5587 - root_mean_squared_error: 0.7475 - val_loss: 0.5444 - val_root_mean_squared_error: 0.7378\n","Epoch 5/500\n","63/63 [==============================] - 2s 38ms/step - loss: 0.5557 - root_mean_squared_error: 0.7454 - val_loss: 0.5418 - val_root_mean_squared_error: 0.7361\n","Epoch 6/500\n","63/63 [==============================] - 2s 39ms/step - loss: 0.5543 - root_mean_squared_error: 0.7445 - val_loss: 0.5427 - val_root_mean_squared_error: 0.7367\n","Epoch 7/500\n","63/63 [==============================] - 2s 38ms/step - loss: 0.5543 - root_mean_squared_error: 0.7445 - val_loss: 0.5420 - val_root_mean_squared_error: 0.7362\n","Epoch 8/500\n","63/63 [==============================] - 2s 38ms/step - loss: 0.5526 - root_mean_squared_error: 0.7434 - val_loss: 0.5407 - val_root_mean_squared_error: 0.7353\n","Epoch 9/500\n","63/63 [==============================] - 2s 38ms/step - loss: 0.5539 - root_mean_squared_error: 0.7443 - val_loss: 0.5433 - val_root_mean_squared_error: 0.7371\n","Epoch 10/500\n","63/63 [==============================] - 2s 40ms/step - loss: 0.5533 - root_mean_squared_error: 0.7438 - val_loss: 0.5409 - val_root_mean_squared_error: 0.7355\n","Epoch 11/500\n","63/63 [==============================] - 2s 40ms/step - loss: 0.5557 - root_mean_squared_error: 0.7454 - val_loss: 0.5406 - val_root_mean_squared_error: 0.7352\n","Epoch 12/500\n","63/63 [==============================] - 3s 43ms/step - loss: 0.5530 - root_mean_squared_error: 0.7436 - val_loss: 0.5404 - val_root_mean_squared_error: 0.7351\n","Epoch 13/500\n","63/63 [==============================] - 2s 40ms/step - loss: 0.5529 - root_mean_squared_error: 0.7435 - val_loss: 0.5404 - val_root_mean_squared_error: 0.7352\n","Epoch 14/500\n","63/63 [==============================] - 3s 40ms/step - loss: 0.5584 - root_mean_squared_error: 0.7473 - val_loss: 0.5439 - val_root_mean_squared_error: 0.7375\n","Epoch 15/500\n","63/63 [==============================] - 2s 39ms/step - loss: 0.5526 - root_mean_squared_error: 0.7433 - val_loss: 0.5403 - val_root_mean_squared_error: 0.7350\n","Epoch 16/500\n","63/63 [==============================] - 2s 39ms/step - loss: 0.5546 - root_mean_squared_error: 0.7447 - val_loss: 0.5435 - val_root_mean_squared_error: 0.7372\n","Epoch 17/500\n","63/63 [==============================] - 2s 39ms/step - loss: 0.5516 - root_mean_squared_error: 0.7427 - val_loss: 0.5466 - val_root_mean_squared_error: 0.7393\n","Epoch 18/500\n","63/63 [==============================] - 2s 39ms/step - loss: 0.5517 - root_mean_squared_error: 0.7427 - val_loss: 0.5395 - val_root_mean_squared_error: 0.7345\n","Epoch 19/500\n","63/63 [==============================] - 2s 39ms/step - loss: 0.5519 - root_mean_squared_error: 0.7429 - val_loss: 0.5459 - val_root_mean_squared_error: 0.7389\n","Epoch 20/500\n","63/63 [==============================] - 2s 39ms/step - loss: 0.5508 - root_mean_squared_error: 0.7422 - val_loss: 0.5399 - val_root_mean_squared_error: 0.7348\n","Epoch 21/500\n","63/63 [==============================] - 2s 40ms/step - loss: 0.5510 - root_mean_squared_error: 0.7423 - val_loss: 0.5401 - val_root_mean_squared_error: 0.7349\n"]}],"source":["rec_system.recc_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oiJRDTa8Ykey","outputId":"bd1061f4-a429-4cee-b930-b457d051028c"},"outputs":[{"name":"stdout","output_type":"stream","text":["10/10 [==============================] - 1s 13ms/step - loss: 0.5779 - root_mean_squared_error: 0.7602\n"]}],"source":["rec_system.eval(rec_system.model) # evaluate the model on the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnCp_OExYkey"},"outputs":[],"source":["a,b = rec_system.model.predict(rec_system.test_data_x,batch_size=rec_system.bs),rec_system.test_data_y  # create prediction and label pairs\n","\n","def add(x,y): return list([float(x)]) + list([float(y)]) # sort prediction and label pairs\n","*pred_pairs, = map(add,a[0],b[0])\n","\n","pred_pairs = list(pred_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0bez4vWC3D_","outputId":"dfd90d06-4cb2-4261-99ea-efb316235c23"},"outputs":[{"data":{"text/plain":["[[3.2046353816986084, 5.0],\n"," [3.7242283821105957, 4.0],\n"," [3.743262767791748, 3.0],\n"," [3.4734957218170166, 5.0],\n"," [3.4189014434814453, 5.0],\n"," [3.506932258605957, 3.0],\n"," [3.4454028606414795, 3.0],\n"," [3.433408498764038, 5.0],\n"," [3.652507781982422, 5.0],\n"," [3.4825100898742676, 3.0]]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["pred_pairs[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZjNihVZC3EA","outputId":"05c4dc6f-d54c-4d44-f301-87984c83c29c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 615 Complete [00h 00m 15s]\n","val_loss: 0.5495085120201111\n","\n","Best val_loss So Far: 0.541039764881134\n","Total elapsed time: 00h 02m 43s\n","\n","Search: Running Trial #616\n","\n","Value             |Best Value So Far |Hyperparameter\n","3                 |4                 |layers\n","0.34145           |0.39063           |dropout_rate\n","selu              |gelu              |final_activation\n","0.070598          |0.00097747        |lr\n","348               |319               |units\n","19                |7                 |tuner/epochs\n","0                 |0                 |tuner/initial_epoch\n","3                 |4                 |tuner/bracket\n","0                 |0                 |tuner/round\n","\n","Epoch 1/19\n","63/63 [==============================] - ETA: 0s - loss: 0.8753"]}],"source":["rec_system.keras_tuner(search=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6bro6OIYkez","outputId":"c289740a-e4b2-4730-bf0f-ba5621db1443"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/500\n","63/63 [==============================] - 9s 100ms/step - loss: 1.1561 - root_mean_squared_error: 1.0752 - val_loss: 0.6028 - val_root_mean_squared_error: 0.7764\n","Epoch 2/500\n","63/63 [==============================] - 6s 90ms/step - loss: 0.6043 - root_mean_squared_error: 0.7774 - val_loss: 0.5609 - val_root_mean_squared_error: 0.7489\n","Epoch 3/500\n","63/63 [==============================] - 6s 91ms/step - loss: 0.5739 - root_mean_squared_error: 0.7576 - val_loss: 0.5489 - val_root_mean_squared_error: 0.7408\n","Epoch 4/500\n","63/63 [==============================] - 6s 91ms/step - loss: 0.5702 - root_mean_squared_error: 0.7551 - val_loss: 0.5489 - val_root_mean_squared_error: 0.7408\n","Epoch 5/500\n","63/63 [==============================] - 6s 92ms/step - loss: 0.5947 - root_mean_squared_error: 0.7712 - val_loss: 0.5443 - val_root_mean_squared_error: 0.7378\n","Epoch 6/500\n","63/63 [==============================] - 6s 95ms/step - loss: 0.5608 - root_mean_squared_error: 0.7489 - val_loss: 0.5416 - val_root_mean_squared_error: 0.7359\n","Epoch 7/500\n","63/63 [==============================] - 6s 90ms/step - loss: 0.5594 - root_mean_squared_error: 0.7479 - val_loss: 0.5538 - val_root_mean_squared_error: 0.7442\n","Epoch 8/500\n","63/63 [==============================] - 6s 90ms/step - loss: 0.5601 - root_mean_squared_error: 0.7484 - val_loss: 0.5460 - val_root_mean_squared_error: 0.7389\n","Epoch 9/500\n","63/63 [==============================] - 6s 90ms/step - loss: 0.5574 - root_mean_squared_error: 0.7466 - val_loss: 0.5417 - val_root_mean_squared_error: 0.7360\n"]}],"source":["rec_system.keras_tuner(search=False) #"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"RNN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}